---
title: "Transcription"
sidebarTitle: "Transcription"
description: "Configure speech-to-text providers for real-time transcription in voice calls."
---

The `transcriber` object controls how spoken audio is converted to text. Accurate, low-latency transcription is critical for voice AI -- the agent cannot respond until it understands what the user said.

## Supported Providers

| Provider | Value | Highlights |
|----------|-------|------------|
| Deepgram | `deepgram` | Fastest, best for real-time voice (recommended) |
| AssemblyAI | `assemblyai` | High accuracy, good entity recognition |
| Google | `google` | Wide language support |
| OpenAI | `openai` | Whisper-based, high accuracy |
| Azure | `azure` | Enterprise-grade, custom models |

## Configuration

```json
{
  "transcriber": {
    "provider": "deepgram",
    "model": "nova-2",
    "language": "en-US",
    "keywords": ["Acme", "AcmePro", "CloudSync"]
  }
}
```

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `provider` | `string` | Yes | - | STT provider: `deepgram`, `assemblyai`, `google`, `openai`, `azure`. |
| `model` | `string` | No | Provider default | STT model to use (e.g., `nova-2` for Deepgram). |
| `language` | `string` | No | `en-US` | BCP-47 language code. |
| `keywords` | `string[]` | No | - | Words to boost for better recognition accuracy. |

<Note>
  The transcriber is optional. If omitted, Relay Agent defaults to Deepgram with the `nova-2` model and `en-US` language.
</Note>

## Deepgram (Recommended)

Deepgram offers the lowest latency and best real-time streaming support, making it the default choice for voice AI.

### Models

| Model | Latency | Accuracy | Notes |
|-------|---------|----------|-------|
| `nova-2` | Fastest | Excellent | Recommended for real-time voice |
| `nova` | Fast | Very good | Previous generation |
| `enhanced` | Medium | High | Better for noisy audio |
| `base` | Fast | Good | Lowest cost |

```json
{
  "transcriber": {
    "provider": "deepgram",
    "model": "nova-2",
    "language": "en-US"
  }
}
```

### Keyword Boosting

Deepgram supports keyword boosting to improve recognition of domain-specific terms. This is particularly useful for company names, product names, and technical jargon that the model might not recognize well out of the box.

```json
{
  "transcriber": {
    "provider": "deepgram",
    "model": "nova-2",
    "language": "en-US",
    "keywords": [
      "Acme",
      "AcmePro",
      "CloudSync",
      "TurboWidget"
    ]
  }
}
```

<Tip>
  Add any proper nouns, product names, or industry terms that callers frequently use. This can dramatically improve recognition accuracy for these words.
</Tip>

## AssemblyAI

AssemblyAI provides high accuracy with strong entity recognition capabilities.

```json
{
  "transcriber": {
    "provider": "assemblyai",
    "language": "en-US"
  }
}
```

## Google Speech-to-Text

Google offers broad language support with over 120 languages and variants.

```json
{
  "transcriber": {
    "provider": "google",
    "model": "latest_long",
    "language": "en-US"
  }
}
```

## OpenAI Whisper

OpenAI's Whisper models provide high accuracy, especially for challenging audio conditions.

```json
{
  "transcriber": {
    "provider": "openai",
    "model": "whisper-1",
    "language": "en"
  }
}
```

## Azure Speech Services

Azure provides enterprise-grade STT with options for custom models trained on your specific domain.

```json
{
  "transcriber": {
    "provider": "azure",
    "language": "en-US"
  }
}
```

## Language Support

The `language` field accepts BCP-47 language codes. Common values:

| Code | Language |
|------|----------|
| `en-US` | English (United States) |
| `en-GB` | English (United Kingdom) |
| `es-ES` | Spanish (Spain) |
| `es-MX` | Spanish (Mexico) |
| `fr-FR` | French (France) |
| `de-DE` | German |
| `pt-BR` | Portuguese (Brazil) |
| `ja-JP` | Japanese |
| `zh-CN` | Chinese (Mandarin, Simplified) |
| `ko-KR` | Korean |
| `hi-IN` | Hindi |

<Warning>
  Not all providers support all languages. Check your chosen provider's documentation for the full list of supported language codes.
</Warning>

## Endpointing

Endpointing determines when the transcriber considers the user's utterance complete. This directly affects how quickly the agent can start responding.

Endpointing is configured through the `settings.turnDetection` object on the agent, not on the transcriber itself. See [Turn Detection](/build/turn-detection) for full details.

The transcriber emits two key signals that the turn detection system uses:

- **`speech_final`**: A segment of speech has been finalized (interim transcript is committed). The user may still be talking.
- **`utterance_end`**: The user has stopped speaking (silence detected). The full utterance is complete.

The gap between `speech_final` and `utterance_end` is where the [greedy inference](/build/turn-detection) optimization operates, starting LLM inference speculatively before the user finishes speaking.
