---
title: "Latency Optimization"
sidebarTitle: "Latency"
description: "Understand, measure, and optimize the end-to-end audio pipeline latency for faster conversations."
---

## Overview

Latency is the time between when a user finishes speaking and when they hear the agent's first word of response. In voice AI, every millisecond matters -- sub-500ms latency feels natural, while anything above 1 second feels sluggish.

Relay Agent's audio pipeline consists of three sequential stages, each contributing to the total response time:

```
User finishes speaking
        |
        v
  [STT] Speech-to-Text     ~100-200ms
        |
        v
  [LLM] Language Model      ~200-600ms (first token)
        |
        v
  [TTS] Text-to-Speech      ~80-150ms (first chunk)
        |
        v
User hears first word       ~400-950ms total
```

## Latency Metrics

Every completed conversational turn emits a `latency` event through [DevTools](/monitor/devtools) with these fields:

| Metric | Description | Typical Range |
|---|---|---|
| `sttMs` | Time from turn start to final STT transcript. | 100-300ms |
| `llmFirstTokenMs` | Time from STT final to first LLM token. | 200-600ms |
| `ttsFirstChunkMs` | Time from first LLM token to first TTS audio chunk. | 80-150ms |
| `totalMs` | End-to-end: turn start to first audio reaching the caller. | 400-950ms |

### Greedy Inference Metrics

When greedy inference mode is active (the default), additional metrics are available:

| Metric | Description |
|---|---|
| `greedy.speechFinalMs` | Timestamp when the first `speech_final` event fired. |
| `greedy.greedyInvokeMs` | Timestamp when the speculative LLM invocation started. |
| `greedy.greedyCancels` | Number of cancel/restart cycles in this turn. |
| `greedy.commitMs` | Timestamp when the generation was committed (locked in). |
| `greedy.estimatedSavingsMs` | Estimated time saved vs. fixed mode. |

## Greedy Inference Mode

Greedy inference is Relay Agent's primary latency optimization. Instead of waiting for the STT provider to emit a final transcript (which includes an endpointing delay), the pipeline starts LLM generation speculatively when the first `speech_final` event fires.

```
Fixed Mode (traditional):
  User speaks ----[silence gap]----> STT final ----> LLM ----> TTS

Greedy Mode:
  User speaks ----> speech_final ----> LLM starts (speculative)
                    [silence gap]----> STT final
                                       |
                                       v
                                  If text changed: cancel + restart LLM
                                  If text same: commit + continue
```

### How It Works

<Steps>
  <Step title="Early speech detection">
    The STT provider emits `speech_final` events as it detects complete phrases, before the full endpointing delay completes.
  </Step>
  <Step title="Speculative LLM invocation">
    The pipeline immediately starts LLM generation with the partial transcript. RAG context is retrieved in parallel if configured.
  </Step>
  <Step title="Commit or cancel">
    When the final STT transcript arrives: if the text matches, the generation is committed. If the user said more words, the generation is cancelled and restarted with the updated text.
  </Step>
  <Step title="Audio commitment">
    Once the first TTS audio chunk is sent to the caller, the generation is permanently committed -- even if a new `speech_final` arrives, the agent will not restart.
  </Step>
</Steps>

Greedy mode typically saves **100-300ms** per turn by overlapping STT endpointing with LLM generation.

### Configuration

Greedy mode is enabled by default. Configure it in the pipeline settings:

```json
{
  "turnDetection": {
    "mode": "greedy",
    "endpointingMs": 300,
    "utteranceEndMs": 1000,
    "greedyCommitOnFirstAudio": true,
    "maxRestarts": 3
  }
}
```

| Setting | Default | Description |
|---|---|---|
| `mode` | `greedy` | `greedy` or `fixed`. Fixed mode waits for the final STT transcript before invoking the LLM. |
| `endpointingMs` | - | Silence duration (ms) before STT considers the utterance complete. |
| `utteranceEndMs` | - | Maximum utterance length before forced endpoint. |
| `greedyCommitOnFirstAudio` | `true` | Lock in the generation when the first TTS audio is sent. |
| `maxRestarts` | - | Maximum cancel/restart cycles before falling back to fixed behavior. |

## Optimization Strategies

### 1. Choose Fast Models

LLM selection has the largest impact on latency:

| Model | First Token Latency | Quality | Cost |
|---|---|---|---|
| `gpt-4o-mini` | ~200ms | Good | Low |
| `gpt-4o` | ~400ms | Excellent | High |
| `claude-3-5-haiku` | ~250ms | Good | Low |
| `claude-3-5-sonnet` | ~350ms | Excellent | Medium |

<Tip>
  For latency-sensitive use cases (e.g. customer support), start with `gpt-4o-mini` or `claude-3-5-haiku`. Use A/B testing to verify that quality meets your needs before trading up to larger models.
</Tip>

### 2. Optimize Prompts

Shorter system prompts reduce time-to-first-token:

- Keep system prompts under 500 tokens.
- Move detailed instructions into tool descriptions or knowledge bases.
- Avoid long conversation history -- summarize older turns when the context window grows.

### 3. Use Streaming TTS

Relay Agent streams TTS in sentence-sized chunks rather than waiting for the full LLM response. The `sentenceBufferMinChars` setting controls when text is flushed to TTS:

| Setting | Behavior | Tradeoff |
|---|---|---|
| `10` chars | More frequent, smaller TTS requests. | Lower latency, slightly more TTS API calls. |
| `30` chars | Larger chunks, fewer requests. | Higher latency, better prosody. |
| `20` chars (default) | Balanced. | Good starting point. |

Text is flushed on sentence boundaries (`.`, `!`, `?`) or clause breaks (`,`, `-`, `:`) when the buffer exceeds the minimum.

### 4. Select Low-Latency Voice Providers

TTS provider latency varies significantly:

| Provider | First Chunk Latency | Quality |
|---|---|---|
| ElevenLabs (Turbo v2) | ~80ms | Excellent |
| Google Cloud TTS | ~50ms | Good |
| ElevenLabs (Standard) | ~150ms | Excellent |

### 5. Reduce STT Endpointing Delay

The endpointing delay is the silence required before STT considers the user done speaking. Lower values mean faster response, but risk cutting off the user mid-thought:

| Setting | Behavior |
|---|---|
| `200ms` | Very aggressive. Good for yes/no interactions. |
| `300ms` | Balanced. Works for most conversations. |
| `500ms` | Conservative. Better for complex questions. |

### 6. Leverage RAG Wisely

Knowledge base retrieval adds latency. In greedy mode, RAG runs in parallel with the STT-to-LLM handoff, minimizing impact. If RAG retrieval consistently exceeds 200ms, consider:

- Reducing chunk count per query
- Using smaller embedding models
- Caching frequent queries

## Debugging High Latency

Use [DevTools](/monitor/devtools) to identify the bottleneck:

### Step 1: Connect to a live call

```typescript
const ws = new WebSocket(`wss://host/v1/calls/${callId}/inspect`);
ws.send(JSON.stringify({ filter: ["latency"] }));
```

### Step 2: Analyze latency events

```json
{
  "type": "latency",
  "data": {
    "sttMs": 150,
    "llmFirstTokenMs": 520,
    "ttsFirstChunkMs": 90,
    "totalMs": 760
  }
}
```

### Step 3: Identify the bottleneck

| If High | Likely Cause | Fix |
|---|---|---|
| `sttMs` > 300ms | Slow STT provider or high endpointing delay. | Reduce `endpointingMs`, try a different STT provider. |
| `llmFirstTokenMs` > 500ms | Large model or long prompt. | Switch to a smaller model, shorten system prompt. |
| `ttsFirstChunkMs` > 200ms | Slow TTS provider. | Use a turbo/streaming TTS model. |

### Step 4: Check greedy inference impact

When greedy metrics are present, compare:

```json
{
  "greedy": {
    "estimatedSavingsMs": 180,
    "greedyCancels": 0
  }
}
```

- `estimatedSavingsMs > 0` means greedy mode is helping.
- `greedyCancels > 0` means the user continued speaking after speculation started. Occasional cancels are normal; frequent cancels may indicate the endpointing threshold is too aggressive.

## Latency Budget Example

Target: **500ms** total latency (first word of response)

| Stage | Budget | Strategy |
|---|---|---|
| STT | 120ms | Deepgram Nova-2, 250ms endpointing. |
| LLM | 250ms | GPT-4o-mini, concise prompt (300 tokens). |
| TTS | 80ms | ElevenLabs Turbo v2. |
| Overhead | 50ms | Network, encoding, pipeline orchestration. |
| **Total** | **500ms** | Greedy mode saves additional 100-200ms. |

## Next Steps

- [DevTools](/monitor/devtools) -- Real-time latency monitoring
- [Usage](/monitor/usage) -- Compare costs when choosing faster (cheaper) models
- [A/B Testing](/monitor/ab-testing) -- Test latency vs. quality tradeoffs scientifically
