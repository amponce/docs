---
title: "Core Concepts"
sidebarTitle: "Concepts"
description: "Understand the building blocks of Relay Agent"
---

# Core Concepts

This page explains the fundamental building blocks of Relay Agent: how agents,
calls, the audio pipeline, tools, knowledge bases, and other components work
together.

---

## Agents

An **agent** is the core unit in Relay Agent. It defines an AI personality with
a specific model, voice, behavior, and set of tools. Every call is handled by
an agent (or a composition of agents).

### Agent Configuration

| Field | Description |
|-------|------------|
| `name` | Human-readable identifier for the agent |
| `model` | LLM configuration: provider, model name, temperature, max tokens, system prompt |
| `voice` | TTS configuration: provider, voice ID, speed, stability |
| `transcriber` | STT configuration: provider, model, language, keywords for boosting |
| `firstMessage` | What the agent says when the call starts (optional) |
| `tools` | Function calling tools, transfer tools, and agent transfer tools |
| `settings` | Responsiveness, interruption sensitivity, backchanneling, turn detection |
| `memory` | Cross-call memory configuration (per-contact, per-number, or global) |
| `knowledgeBaseIds` | Knowledge bases to attach for RAG retrieval |
| `metadata` | Arbitrary key-value data for your application |

### Supported Providers

<AccordionGroup>
  <Accordion title="LLM Providers">
    | Provider | Example Models |
    |----------|---------------|
    | `openai` | `gpt-4.1`, `gpt-4.1-mini`, `gpt-4o` |
    | `anthropic` | `claude-sonnet-4-20250514`, `claude-haiku-4-20250514` |
    | `google` | `gemini-2.5-pro`, `gemini-2.5-flash` |
    | `groq` | `llama-3.3-70b-versatile`, `mixtral-8x7b-32768` |
    | `together` | `meta-llama/Llama-3.3-70B-Instruct-Turbo` |
  </Accordion>
  <Accordion title="TTS Providers">
    | Provider | Voices |
    |----------|--------|
    | `elevenlabs` | rachel, adam, antoni, bella, domi, elli, josh, sam, and custom clones |
    | `openai` | alloy, echo, fable, onyx, nova, shimmer |
    | `playht` | Hundreds of voices via PlayHT library |
    | `deepgram` | aura-asteria, aura-luna, aura-stella, aura-athena, aura-hera, aura-orion |
    | `cartesia` | Sonic voices with ultra-low latency |
  </Accordion>
  <Accordion title="STT Providers">
    | Provider | Models |
    |----------|--------|
    | `deepgram` | nova-2 (default), nova, enhanced, base |
    | `assemblyai` | best, nano |
    | `google` | latest_long, latest_short |
    | `openai` | whisper-1 |
    | `azure` | Azure Speech Services |
  </Accordion>
</AccordionGroup>

### Agent Versioning

Every update to an agent increments its `version` number and marks it as
unpublished (`isPublished: false`). This lets you test changes before pushing
them live.

```
v1 (published) --> v2 (draft) --> v2 (published) --> v3 (draft) ...
```

Publish an agent to make the current configuration active:

```bash
curl -X POST http://localhost:3001/v1/agents/agent_abc123/publish \
  -H "Authorization: Bearer $RELAY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"versionTitle": "Improved greeting", "versionDescription": "Updated first message"}'
```

---

## Calls

A **call** is a voice conversation session between a human and one or more
agents. Relay Agent supports two call types:

### Phone Calls

Phone calls are routed through Twilio. Audio is encoded in mulaw format at
8kHz for telephony transport.

| Direction | Description |
|-----------|-------------|
| `outbound` | Agent calls a phone number (`POST /v1/calls/phone`) |
| `inbound` | Human calls a Twilio number configured with an inbound agent |

### Web Calls

Web calls use a WebSocket connection for audio. Audio is PCM16 at 16kHz.

<Steps>
  <Step title="Create a web call session">
    `POST /v1/calls/web` returns a `callId`, `accessToken`, and `websocketUrl`.
  </Step>
  <Step title="Connect via WebSocket">
    Open a WebSocket to the returned URL. The `token` query parameter is
    validated against the stored hash.
  </Step>
  <Step title="Stream audio">
    Send JSON messages with `{ type: "audio", audio: "<base64_pcm16>" }`.
    Receive audio, transcripts, and events back.
  </Step>
</Steps>

### Call Lifecycle

```
queued --> ringing --> in_progress --> completed
                  \                \-> failed
                   \-> no_answer
```

| Status | Description |
|--------|-------------|
| `queued` | Call created, not yet initiated |
| `ringing` | Phone is ringing (phone calls only) |
| `in_progress` | Active conversation |
| `completed` | Call ended normally |
| `failed` | Call failed (initiation error, provider error) |
| `no_answer` | Callee did not pick up (phone calls) |

### Call Data

After a call ends, the call record includes:

- **Transcript**: Array of `{ role, content, timestamp }` entries
- **Summary**: Auto-generated summary of the conversation
- **Sentiment**: Overall sentiment analysis (`positive`, `neutral`, `negative`)
- **Cost breakdown**: Per-component costs (STT, TTS, LLM, telephony) in integer cents
- **Latency metrics**: Average STT, LLM, TTS, and total latency per turn
- **Recording URL**: Link to the audio recording (if enabled)

---

## Audio Pipeline

The audio pipeline is the real-time engine that orchestrates STT, LLM, and TTS
for each conversational turn. It processes audio in a streaming fashion to
minimize latency.

### Pipeline Stages

The pipeline moves through four stages for each turn:

```
idle --> listening --> thinking --> speaking --> listening ...
                                      |
                          (interruption) --> listening
```

| Stage | What Happens |
|-------|-------------|
| `idle` | Pipeline is initialized but not processing |
| `listening` | STT is processing user audio, emitting partial and final transcripts |
| `thinking` | LLM is generating a response, streaming tokens |
| `speaking` | TTS is synthesizing audio, streaming chunks to the caller |

### Sentence Buffering

The pipeline does not wait for the entire LLM response before starting TTS.
Instead, it buffers LLM tokens and sends text to TTS at natural boundaries:

1. **Sentence end**: Period, exclamation mark, question mark, or semicolon followed by a space
2. **Clause break**: Comma, dash, or colon when the buffer exceeds 20 characters (configurable via `sentenceBufferMinChars`)
3. **Flush**: Any remaining text is sent when the LLM finishes generating

This produces natural speech cadence while minimizing time-to-first-audio.

### Interruption Handling

If the user speaks while the agent is in the `speaking` stage, the pipeline
detects the interruption and takes action:

<Steps>
  <Step title="Detection">
    STT partial transcripts are monitored during the speaking stage. An
    interruption triggers when the user speaks 2 or more words. This threshold
    filters out echo (the microphone picking up the agent's own TTS output).
  </Step>
  <Step title="Cancellation">
    The LLM generation and TTS synthesis are cancelled. Queued audio on the
    caller side is cleared.
  </Step>
  <Step title="Context Preservation">
    The partial agent response (what was spoken before interruption) is saved
    to conversation history, maintaining coherent context.
  </Step>
  <Step title="Resume Listening">
    The pipeline returns to the `listening` stage to process the user's
    interrupting speech.
  </Step>
</Steps>

---

## Turn Detection

Turn detection determines when the user has finished speaking and the agent
should respond. Relay Agent supports two modes.

### Fixed Mode

In fixed mode, the pipeline waits for the STT provider's `final` transcript
event (usually triggered by a silence timeout). This is simple and reliable
but adds latency equal to the silence detection window.

| Setting | Default | Range | Description |
|---------|---------|-------|-------------|
| `endpointingMs` | 300 | 100-2000 | Silence duration before STT considers speech final |
| `utteranceEndMs` | 1000 | 200-5000 | Maximum silence gap within an utterance |

### Greedy Mode (Default)

Greedy mode reduces response latency by starting LLM inference speculatively
on intermediate `speech_final` events, before the user is fully done speaking.

<Steps>
  <Step title="speech_final fires">
    STT detects a pause and emits a `speech_final` event. The greedy inference
    manager starts an LLM generation with the transcript so far.
  </Step>
  <Step title="User continues speaking">
    If the user resumes speaking, the in-progress LLM generation is cancelled
    and restarted with the updated transcript. This can happen up to
    `maxRestarts` times (default: 3).
  </Step>
  <Step title="utterance_end fires">
    When the STT confirms the user is done, the current generation is
    committed (locked in) and cannot be cancelled.
  </Step>
  <Step title="First audio sent">
    When the first TTS audio chunk is sent to the caller, the generation is
    also committed (if `greedyCommitOnFirstAudio` is true, which is the default).
  </Step>
</Steps>

Greedy mode typically saves 200-500ms of latency per turn compared to fixed
mode, at the cost of occasional wasted LLM tokens when the user continues speaking.

| Setting | Default | Description |
|---------|---------|-------------|
| `mode` | `greedy` | Turn detection mode: `fixed` or `greedy` |
| `greedyCommitOnFirstAudio` | `true` | Lock in generation when first audio plays |
| `maxRestarts` | 3 | Maximum cancel/restart cycles before forced commit |

---

## Tools

Tools let your agent take actions during a conversation: call APIs, transfer
calls, or hand off to another agent. Relay Agent supports three tool types.

### Function Tools

Function tools call external webhooks when the LLM decides to use them. They
follow the OpenAI function calling JSON schema format.

```json
{
  "type": "function",
  "function": {
    "name": "check_order_status",
    "description": "Look up the status of a customer order by order ID",
    "parameters": {
      "type": "object",
      "properties": {
        "orderId": {
          "type": "string",
          "description": "The order ID (e.g., ORD-12345)"
        }
      },
      "required": ["orderId"]
    }
  },
  "server": {
    "url": "https://api.example.com/orders/status",
    "timeoutMs": 5000,
    "retries": 2,
    "headers": {
      "X-Api-Key": "your-service-key"
    }
  },
  "execution": {
    "speakDuringExecution": true,
    "executionMessage": "Let me look that up for you...",
    "speakAfterExecution": true,
    "timeoutMs": 120000
  }
}
```

### Transfer Call Tools

Transfer the caller to a human agent via phone number. Supports warm and cold
transfers.

```json
{
  "type": "transfer_call",
  "name": "transfer_to_support",
  "description": "Transfer the caller to a live support agent",
  "destination": {
    "type": "predefined",
    "number": "+15559876543"
  },
  "transferOption": {
    "type": "warm_transfer",
    "holdMusic": "jazz",
    "handoffPrompt": "Connecting you with a specialist now...",
    "agentDetectionTimeoutMs": 30000
  }
}
```

| Transfer Type | Description |
|--------------|-------------|
| `warm_transfer` | Agent stays on the line until the human agent picks up, provides context, then disconnects |
| `cold_transfer` | Caller is transferred immediately; agent disconnects |

### Agent Transfer Tools

Hand off the conversation to another AI agent within a composition.

```json
{
  "type": "agent_transfer",
  "name": "transfer_to_billing",
  "description": "Transfer to the billing specialist agent",
  "agentId": "agent_billing_123",
  "webhookSetting": "both_agents",
  "postCallAnalysisSetting": "only_destination"
}
```

---

## Knowledge Bases (RAG)

Knowledge bases allow your agent to answer questions using your own documents.
When a user asks a question, relevant chunks are retrieved and injected into
the LLM context.

### How RAG Works

<Steps>
  <Step title="Upload documents">
    Upload PDF, DOCX, or TXT files to a knowledge base. Each file is split
    into chunks (default: 512 tokens with 50-token overlap).
  </Step>
  <Step title="Generate embeddings">
    Each chunk is embedded using the configured model (default:
    `text-embedding-3-small`, 1536 dimensions, stored as pgvector).
  </Step>
  <Step title="Retrieval at conversation time">
    When the user speaks, the transcribed text is embedded and compared against
    stored chunks using cosine similarity.
  </Step>
  <Step title="Context injection">
    The top-K most relevant chunks (above the similarity threshold) are
    injected as a system message before the user's message in the LLM context.
  </Step>
</Steps>

### Knowledge Base Settings

| Setting | Default | Description |
|---------|---------|-------------|
| `embeddingModel` | `text-embedding-3-small` | Model for generating embeddings |
| `chunkSize` | 512 | Target chunk size in tokens |
| `chunkOverlap` | 50 | Overlap between adjacent chunks |

### Agent RAG Settings

Configure retrieval behavior per-agent in the `settings.rag` field:

| Setting | Default | Range | Description |
|---------|---------|-------|-------------|
| `topK` | 3 | 1-10 | Number of chunks to retrieve |
| `threshold` | 0.7 | 0-1 | Minimum similarity score |
| `contextTemplate` | `[RELEVANT CONTEXT]\n{{chunks}}\n[/RELEVANT CONTEXT]` | - | Template for injecting chunks |

<Tip>
You can attach up to 10 knowledge bases to a single agent using the
`knowledgeBaseIds` array.
</Tip>

---

## Compositions

A **composition** (or composed agent) orchestrates multiple specialized agents
within a single call. A router agent determines which specialist should handle
the current part of the conversation.

### Composition Structure

```json
{
  "name": "Customer Service Team",
  "routerAgentId": "agent_router",
  "agents": [
    { "id": "agent_router", "role": "router" },
    { "id": "agent_sales", "role": "sales" },
    { "id": "agent_support", "role": "support" },
    { "id": "agent_billing", "role": "billing" }
  ],
  "transitions": [
    { "from": "agent_router", "to": "agent_sales", "condition": "sales_inquiry" },
    { "from": "agent_router", "to": "agent_support", "condition": "technical_issue" },
    { "from": "agent_router", "to": "agent_billing", "condition": "billing_question" },
    { "from": "agent_sales", "to": "agent_router", "condition": "done" },
    { "from": "agent_support", "to": "agent_router", "condition": "done" },
    { "from": "agent_billing", "to": "agent_router", "condition": "done" }
  ],
  "sharedContext": true,
  "seamlessHandoff": true
}
```

| Field | Description |
|-------|-------------|
| `routerAgentId` | The agent that receives the call first and routes to specialists |
| `agents` | Array of agent IDs and their roles within the composition |
| `transitions` | Rules for when and how agents hand off to each other |
| `sharedContext` | When true, conversation history is shared across all agents in the composition |
| `seamlessHandoff` | When true, transitions happen without audible gaps or announcements |

<Note>
Transitions use condition strings that the router agent evaluates. These are
typically triggered by the LLM calling an `agent_transfer` tool or by
detecting intent in the conversation.
</Note>

---

## Campaigns

A **campaign** is a batch outbound calling operation. It takes a list of phone
numbers, an agent, a schedule, and a retry policy, then processes calls
automatically using BullMQ queues.

### Campaign Lifecycle

```
draft --> active --> paused --> active --> completed
                           \-> cancelled
```

| Status | Description |
|--------|-------------|
| `draft` | Created but not started. Add contacts during this phase. |
| `active` | Calls are being placed according to the schedule |
| `paused` | Temporarily stopped; can be resumed |
| `completed` | All contacts have been called |
| `cancelled` | Permanently stopped |

### Schedule Configuration

| Setting | Description |
|---------|-------------|
| `startAt` | ISO 8601 timestamp to begin the campaign |
| `endAt` | ISO 8601 timestamp deadline (optional) |
| `callingHours` | Time window for placing calls (e.g., `09:00` to `17:00`) |
| `daysOfWeek` | Array of day numbers (0=Sunday, 6=Saturday) |
| `maxConcurrent` | Maximum simultaneous calls |
| `callsPerMinute` | Throttle rate |

### Retry Policy

| Setting | Default | Description |
|---------|---------|-------------|
| `maxAttempts` | 1 | Maximum call attempts per contact |
| `delayMinutes` | 60 | Wait time between retry attempts |
| `retryOn` | `[]` | Conditions to retry on: `no_answer`, `busy`, `failed` |

---

## Webhooks

Relay Agent sends real-time webhook notifications for call lifecycle events.
All payloads are signed with HMAC-SHA256 using your `WEBHOOK_SIGNING_SECRET`.

### Event Types

| Event | Description |
|-------|-------------|
| `call.created` | A new call has been created |
| `call.started` | The call is now in progress |
| `call.ended` | The call has ended |
| `call.failed` | The call failed to connect |
| `call.transferred` | The call was transferred to another number or agent |
| `call.whisper` | A whisper message was injected |
| `call.interject` | An interjection was sent |
| `call.terminate` | A termination request was sent |
| `tool.invoked` | A tool was called during the conversation |
| `tool.response` | A tool returned a response |
| `transcript.partial` | Partial STT transcript |
| `transcript.final` | Final STT transcript |
| `memory.update` | Contact memory was updated |
| `agent.transitioned` | Agent hand-off in a composition |

### Webhook Payload

```json
{
  "id": "evt_abc123",
  "type": "call.ended",
  "callId": "call_xyz789",
  "agentId": "agent_abc123",
  "orgId": "org_default",
  "timestamp": "2026-02-08T15:30:00.000Z",
  "data": {
    "duration_ms": 45000,
    "end_reason": "caller_hung_up",
    "sentiment": "positive"
  }
}
```

### Verifying Signatures

```typescript
import { createHmac, timingSafeEqual } from "node:crypto";

function verifyWebhook(
  payload: string,
  signature: string,
  secret: string
): boolean {
  const expected = createHmac("sha256", secret).update(payload).digest("hex");
  return timingSafeEqual(
    Buffer.from(signature, "hex"),
    Buffer.from(expected, "hex")
  );
}
```

<Warning>
Always verify webhook signatures before processing payloads. An attacker could
forge webhook deliveries if you skip verification.
</Warning>

---

## Dynamic Variables

Dynamic variables let you personalize agent prompts per-call using `{{mustache}}`
syntax. Pass variables when creating a call, and they are substituted into the
agent's system prompt.

**System prompt:**
```
You are calling {{customerName}} about order {{orderId}}.
Their plan is {{planType}}.
```

**Call creation:**
```json
{
  "agentId": "agent_abc123",
  "from": "+15551234567",
  "to": "+15559876543",
  "dynamicVariables": {
    "customerName": "Jane Doe",
    "orderId": "ORD-12345",
    "planType": "Enterprise"
  }
}
```

**Resolved prompt:**
```
You are calling Jane Doe about order ORD-12345.
Their plan is Enterprise.
```

---

## Memory

Agent memory allows your agents to remember information across multiple calls
with the same contact. This creates more natural, personalized conversations.

| Strategy | Key | Description |
|----------|-----|-------------|
| `per-contact` | Caller phone number or contact ID | Most common; separate memory per individual |
| `per-number` | Twilio number | Shared memory for everyone who calls the same number |
| `global` | Agent ID | Single shared memory for all callers |

### Memory Configuration

```json
{
  "memory": {
    "enabled": true,
    "strategy": "per-contact",
    "retentionDays": 90,
    "maxTokens": 2000,
    "autoSummarize": true
  }
}
```

When `autoSummarize` is enabled, the system generates a summary of each call
and accumulates facts about the contact. This context is injected into future
calls with the same contact.

---

## Architecture Diagram

```
                                   +------------------+
                                   |  Admin Console   |
                                   |  (React + Vite)  |
                                   +--------+---------+
                                            |
                                   Session Auth (Cookie)
                                            |
+------------------+               +--------+---------+
|  Your Backend    |  API Key Auth |                  |
|  (SDK / REST)    +-------------->+  Fastify Server  |
+------------------+               |  (HTTP + WS)     |
                                   +--------+---------+
                                            |
                         +------------------+------------------+
                         |                  |                  |
                  +------+------+   +-------+-------+  +------+------+
                  |   Agents    |   |    Calls      |  |  Knowledge  |
                  |   Engine    |   | Orchestrator  |  |    Bases    |
                  +------+------+   +-------+-------+  +------+------+
                         |                  |                  |
                  +------+------------------+------------------+------+
                  |                                                   |
           +------+------+                                    +------+------+
           | Audio Pipeline                                   |   Event Bus  |
           | STT -> LLM -> TTS                                |  + Webhooks  |
           +------+------+                                    +------+------+
                  |                                                  |
    +-------------+-------------+                           +-------+-------+
    |             |             |                            |               |
+---+---+   +----+----+  +----+----+                 +------+------+ +------+------+
|  STT  |   |   LLM   |  |   TTS   |                | PostgreSQL  | |    Redis    |
|Provider|   | Provider|  | Provider|                | (Drizzle)   | |  (BullMQ)  |
+-------+   +---------+  +---------+                +-------------+ +-------------+
```
